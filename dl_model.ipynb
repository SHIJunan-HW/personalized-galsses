{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41af2e4b-329a-42b0-9a23-155015cb1b4e",
   "metadata": {},
   "source": [
    "# 环境设置"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7f31907-c305-433d-b0ee-917839096807",
   "metadata": {},
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "from numpy.linalg import inv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import pickle\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "# 设置中文字体（全局设置）\n",
    "\n",
    "font_path = r\"C:\\Users\\34568\\simhei.ttf\"  #字体路径\n",
    "chinese_font = FontProperties(fname=font_path)\n",
    "mpl.rcParams[\"font.family\"] = chinese_font.get_name()  # 设置字体名称\n",
    "mpl.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "base_working_dir = r'C:\\Users\\34568\\Desktop\\GlassesGAN'\n",
    "repo_name_dir = 'GlassesGAN_release'\n",
    "msvc_path = r\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\bin\\Hostx64\\x64\"\n",
    "os.environ[\"PATH\"] = msvc_path + \";\" + os.environ.get(\"PATH\", \"\")\n",
    "predictor_path = r\"C:\\Users\\34568\\Desktop\\GlassesGAN\\GlassesGAN_release\\encoder4editing\\shape_predictor_68_face_landmarks.dat\"\n",
    "image_path = r\"imgtest/002.jpg\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726d8e4a-a5de-4c7c-885f-aa1b0f4ca150",
   "metadata": {},
   "source": [
    "# 获取人脸关键参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25efebe-763b-4c85-bbbd-a3e7eb28d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 detector 和 predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)  # predictor_path 是模型路径\n",
    "\n",
    "# 读取图像\n",
    "image = cv2.imread(image_path)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 检测人脸\n",
    "faces = detector(gray)\n",
    "\n",
    "for face in faces:\n",
    "    landmarks = predictor(gray, face)\n",
    "\n",
    "    # 提取所有 68 个关键点坐标，生成一个长度为 136 的向量\n",
    "    face_features = np.zeros(136)\n",
    "    for i in range(68):\n",
    "        face_features[2 * i] = landmarks.part(i).x     # x 坐标\n",
    "        face_features[2 * i + 1] = landmarks.part(i).y # y 坐标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b876ff1-c223-4578-9e73-ae4c30b90fba",
   "metadata": {},
   "source": [
    "# 图像初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98586f2-ef9b-4424-8aee-c113402834fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: pretrained_models/e4e_ffhq_encode.pt\n",
      "Model successfully loaded!\n",
      "Model checkpoint: C:\\Users\\34568\\Desktop\\GlassesGAN/deeplab_epoch_19.pth (valid)\n",
      "DeepLab script: C:\\Users\\34568\\Desktop\\GlassesGAN/GlassesGAN_release/datasetGAN_release/datasetGAN\\test_deeplab_cross_validation.py (valid)\n",
      "\n",
      "Removing temporary folder\n",
      "\n",
      "Copying model to temporary working directory\n",
      "Aligned image has shape: (1024, 1024)\n",
      "Inference took 1.2115 seconds.\n",
      "\n",
      "Saving image and fake groundtruth maps to temporary file location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:01,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:01,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:02,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:02,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:03,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:03,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:04,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:04,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:05,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:05,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:05,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:06,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:06,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:07,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:07,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:08,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:08,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:08,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running deeplab\n",
      "Opt {'exp_dir': 'C:\\\\Users\\\\34568\\\\Desktop\\\\GlassesGAN/tempfolder/model', 'batch_size': 64, 'category': 'face', 'debug': False, 'dim': [512, 512, 5088], 'deeplab_res': 512, 'number_class': 4, 'testing_data_number_class': 4, 'max_training': 7, 'stylegan_ver': '1', 'annotation_data_from_w': False, 'annotation_mask_path': './custom_data/annotation/training_data', 'testing_path': 'C:\\\\Users\\\\34568\\\\Desktop\\\\GlassesGAN/tempfolder/input_images', 'average_latent': './custom_data/training_latent/avg_latent_stylegan1.npy', 'annotation_image_latent_path': './custom_data/training_latent/latent_stylegan1.npy', 'stylegan_checkpoint': './checkpoints/stylegan_pretrain/karras2019stylegan-ffhq-1024x1024_old_serialization.pt', 'model_num': 10, 'upsample_mode': 'bilinear'}\n",
      "args Namespace(exp='C:\\\\Users\\\\34568\\\\Desktop\\\\GlassesGAN/tempfolder/face_34.json', resume='C:\\\\Users\\\\34568\\\\Desktop\\\\GlassesGAN/tempfolder/model', cross_validate=False, validation_number=0, chosen_deeplab_epoch=19)\n",
      "Report performance on the best checkpoint\n",
      "Val Data length, 20\n",
      "Testing Data length, 20\n",
      "\n",
      "\n",
      "Loading the processed images\n",
      "\n",
      "Saving image and fake groundtruth maps to temporary file location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image to (512,512,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running deeplab\n",
      "Opt {'exp_dir': 'C:\\\\Users\\\\34568\\\\Desktop\\\\GlassesGAN/tempfolder/model', 'batch_size': 64, 'category': 'face', 'debug': False, 'dim': [512, 512, 5088], 'deeplab_res': 512, 'number_class': 4, 'testing_data_number_class': 4, 'max_training': 7, 'stylegan_ver': '1', 'annotation_data_from_w': False, 'annotation_mask_path': './custom_data/annotation/training_data', 'testing_path': 'C:\\\\Users\\\\34568\\\\Desktop\\\\GlassesGAN/tempfolder/input_images', 'average_latent': './custom_data/training_latent/avg_latent_stylegan1.npy', 'annotation_image_latent_path': './custom_data/training_latent/latent_stylegan1.npy', 'stylegan_checkpoint': './checkpoints/stylegan_pretrain/karras2019stylegan-ffhq-1024x1024_old_serialization.pt', 'model_num': 10, 'upsample_mode': 'bilinear'}\n",
      "args Namespace(exp='C:\\\\Users\\\\34568\\\\Desktop\\\\GlassesGAN/tempfolder/face_34.json', resume='C:\\\\Users\\\\34568\\\\Desktop\\\\GlassesGAN/tempfolder/model', cross_validate=False, validation_number=0, chosen_deeplab_epoch=19)\n",
      "Report performance on the best checkpoint\n",
      "Val Data length, 1\n",
      "Testing Data length, 1\n",
      "\n",
      "\n",
      "Loading the processed images\n"
     ]
    }
   ],
   "source": [
    "# 切换到项目目录并导入核心类\n",
    "os.chdir(f'{base_working_dir}/{repo_name_dir}')\n",
    "from glasses_vton_inference import glasses_vton_inference\n",
    "\n",
    "# 初始化参数\n",
    "glasses_option = 'RG'\n",
    "chosen_deeplab_epoch = 19\n",
    "fitted_pca_fp = f'{base_working_dir}/fitted_pca_celebhq_dataset_results.joblib'\n",
    "ave_add_glasses_diff_fp = f'{base_working_dir}/aveglassesdiff_celebhq_dataset_results_RG.npy'\n",
    "resume_model_ckpt = f'{base_working_dir}/deeplab_epoch_{chosen_deeplab_epoch}.pth'\n",
    "\n",
    "# 初始化 GlassesVton 类实例\n",
    "gvton = glasses_vton_inference(\n",
    "    fitted_pca_fp=fitted_pca_fp,\n",
    "    ave_add_glasses_diff_fp=ave_add_glasses_diff_fp,\n",
    "    base_working_dir=base_working_dir,\n",
    "    temp_save_folder=f'{base_working_dir}/tempfolder',\n",
    "    resume_model_ckpt=resume_model_ckpt,\n",
    "    deeplab_script_location=f'{base_working_dir}/GlassesGAN_release/datasetGAN_release/datasetGAN',\n",
    "    chosen_deeplab_epoch=chosen_deeplab_epoch,\n",
    "    load_loc=base_working_dir,\n",
    "    use_full_glasses_or_frames_mask='frames',\n",
    "    run_tests=False,\n",
    "    outer_dilation_factor=5,\n",
    "    outer_blur_factor=12,\n",
    "    inner_blur_factor=5,\n",
    "    ideal_avg_glasses_frame_area=0.020,\n",
    "    auto_clean=True\n",
    ")\n",
    "input_image, result_image, base_latent = gvton.embed_input_image(image_path, show_plots=False)\n",
    "start_latent, edit_image, _ = gvton.add_avg_glasses(input_image, base_latent, base_bias=1, show_plots=False, auto_pick_bias=True, return_blended_image=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8933e8-d2d2-4baa-b10d-687ca49251da",
   "metadata": {},
   "source": [
    "# 基于上下文老虎机的推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f665f21-7ff3-48c3-97c5-f929e9a4ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)  # Output mean and log variance\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        mean, log_var = out[:, :1], out[:, 1:]  # Split mean and log variance\n",
    "        return mean, log_var\n",
    "\n",
    "\n",
    "def heteroscedastic_loss(y_true, mean_pred, log_var_pred):\n",
    "    \"\"\"Heteroscedastic loss function (considering prediction uncertainty)\"\"\"\n",
    "    precision = torch.exp(-log_var_pred)  # Precision = 1/variance\n",
    "    return torch.mean(precision * (y_true - mean_pred) **2 + log_var_pred)\n",
    "\n",
    "\n",
    "class GlassesRecommender:\n",
    "    \"\"\"Glasses recommendation system (based on deep contextual bandit)\"\"\"\n",
    "    def __init__(self, param_ranges=None, alpha=0.2, load_path=None):\n",
    "        # Glasses parameter ranges (6 parameters)\n",
    "        self.param_ranges = param_ranges or [\n",
    "            (-5.0, 10.0),    # Size\n",
    "            (-10.0, 4.0),    # Height\n",
    "            (-4.0, 20.0),    # Squareness\n",
    "            (-20.0, 20.0),   # Round_Shrink\n",
    "            (-15.0, 20.0),   # Cateye\n",
    "            (-10.0, 10.0)    # Thicken\n",
    "        ]\n",
    "        self.param_names = [\"Size\", \"Height\", \"Squareness\", \"Round_Shrink\", \"Cateye\", \"Thicken\"]\n",
    "        self.alpha = alpha  # UCB exploration coefficient\n",
    "        \n",
    "        # Input dimensions: facial features are 68 key points (x,y), total 136 dimensions\n",
    "        self.context_dim = 136  # 68*2 = 136\n",
    "        self.param_dim = 6      # Glasses parameter dimensions\n",
    "        # Expanded feature dimensions = facial features + glasses parameters + interaction terms (facial features × glasses parameters)\n",
    "        self.expanded_dim = self.context_dim + self.param_dim + self.context_dim * self.param_dim\n",
    "\n",
    "        # Neural network configuration\n",
    "        self.device = torch.device(\"cuda\")  # Can be changed to \"cuda\" to enable GPU\n",
    "        self.model = RewardModel(self.expanded_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "        # Data cache\n",
    "        self.data = []  # Store (feature vector, reward)\n",
    "        self.history = []  # Store complete history\n",
    "\n",
    "        # Load pre-trained model\n",
    "        if load_path and os.path.exists(load_path):\n",
    "            self.load_model(load_path)\n",
    "        else:\n",
    "            self.model.apply(self._init_weights)  # Initialize weights\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"Initialize neural network weights\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_normal_(m.weight)  # Xavier initialization\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)  # Initialize bias to 0\n",
    "\n",
    "    def build_feature_vector(self, face_features, glass_params):\n",
    "        \"\"\"Build feature vector (including interaction terms)\"\"\"\n",
    "        # Dimension check\n",
    "        if len(face_features) != self.context_dim:\n",
    "            raise ValueError(f\"Facial feature dimension error: expected {self.context_dim}, got {len(face_features)}\")\n",
    "        if len(glass_params) != self.param_dim:\n",
    "            raise ValueError(f\"Glasses parameter dimension error: expected {self.param_dim}, got {len(glass_params)}\")\n",
    "        \n",
    "        # Base features = facial features + glasses parameters\n",
    "        base_features = np.concatenate([face_features, glass_params])\n",
    "        # Interaction terms = facial features × glasses parameters (outer product expansion)\n",
    "        interaction_terms = np.outer(face_features, glass_params).flatten()\n",
    "        return np.concatenate([base_features, interaction_terms])\n",
    "\n",
    "    def generate_candidate_params(self, n_candidates=50, strategy='lhs'):\n",
    "        \"\"\"Generate candidate glasses parameters (sampling strategy)\"\"\"\n",
    "        if strategy == 'lhs':\n",
    "            # Latin Hypercube Sampling (more uniform space coverage)\n",
    "            candidates = np.zeros((n_candidates, self.param_dim))\n",
    "            for i, (low, high) in enumerate(self.param_ranges):\n",
    "                segments = np.linspace(0, 1, n_candidates + 1)\n",
    "                points = np.random.uniform(segments[:-1], segments[1:])  # Random points in each interval\n",
    "                np.random.shuffle(points)  # Shuffle order\n",
    "                candidates[:, i] = low + points * (high - low)  # Map to parameter range\n",
    "            return candidates\n",
    "        else:\n",
    "            # Random uniform sampling\n",
    "            return np.array([\n",
    "                [np.random.uniform(low, high) for (low, high) in self.param_ranges] \n",
    "                for _ in range(n_candidates)\n",
    "            ])\n",
    "\n",
    "    def recommend(self, face_features, n_candidates=50, return_all=False):\n",
    "        \"\"\"Recommend optimal glasses parameters (based on UCB strategy)\"\"\"\n",
    "        if face_features is None:\n",
    "            raise ValueError(\"Facial features cannot be empty (face_features is None)\")\n",
    "        \n",
    "        # Generate candidate parameters\n",
    "        candidate_params = self.generate_candidate_params(n_candidates)\n",
    "        scores = []\n",
    "        \n",
    "        # Calculate UCB score for each candidate\n",
    "        with torch.no_grad():  # Turn off gradient calculation\n",
    "            for params in candidate_params:\n",
    "                try:\n",
    "                    feature_vector = self.build_feature_vector(face_features, params)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Feature construction failed: {e}\")\n",
    "                    continue\n",
    "                feature_tensor = torch.FloatTensor(feature_vector).unsqueeze(0).to(self.device)\n",
    "                mean, log_var = self.model(feature_tensor)\n",
    "                std = torch.exp(0.5 * log_var)  # Standard deviation = exp(0.5*log_var)\n",
    "                ucb_score = mean.item() + self.alpha * std.item()  # UCB score = mean + α×standard deviation\n",
    "                scores.append(ucb_score)\n",
    "        \n",
    "        # Select optimal candidate\n",
    "        best_idx = np.argmax(scores)\n",
    "        if return_all:\n",
    "            return candidate_params[best_idx], candidate_params, scores\n",
    "        else:\n",
    "            return candidate_params[best_idx]\n",
    "\n",
    "    def update(self, face_features, glass_params, reward):\n",
    "        \"\"\"Update model with user feedback\"\"\"\n",
    "        try:\n",
    "            feature_vector = self.build_feature_vector(face_features, glass_params)\n",
    "        except ValueError as e:\n",
    "            print(f\"Update failed: {e}\")\n",
    "            return\n",
    "        \n",
    "        # Cache data\n",
    "        self.data.append((feature_vector, reward))\n",
    "        self._train_model()  # Train model\n",
    "        self.history.append({\n",
    "            'face_features': face_features, \n",
    "            'glass_params': glass_params, \n",
    "            'reward': reward\n",
    "        })\n",
    "        print(f\"Model updated: reward value={reward}, total samples={len(self.data)}\")\n",
    "\n",
    "    def _train_model(self, epochs=5, batch_size=16):\n",
    "        \"\"\"Train reward model\"\"\"\n",
    "        if len(self.data) < batch_size:\n",
    "            print(f\"Insufficient samples ({len(self.data)} < {batch_size}), skipping training\")\n",
    "            return\n",
    "        \n",
    "        # Prepare data\n",
    "        features, rewards = zip(*self.data)\n",
    "        X = torch.FloatTensor(np.array(features)).to(self.device)\n",
    "        y = torch.FloatTensor(np.array(rewards)).unsqueeze(1).to(self.device)  # Convert to column vector\n",
    "\n",
    "        # Build data loader\n",
    "        dataset = torch.utils.data.TensorDataset(X, y)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Training\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for x_batch, y_batch in loader:\n",
    "                self.optimizer.zero_grad()  # Clear gradients\n",
    "                mean, log_var = self.model(x_batch)\n",
    "                loss = heteroscedastic_loss(y_batch, mean, log_var)\n",
    "                loss.backward()  # Backpropagation\n",
    "                self.optimizer.step()  # Update parameters\n",
    "                total_loss += loss.item()\n",
    "            avg_loss = total_loss / len(loader)\n",
    "        print(f\"Training completed: average loss={avg_loss:.4f}\")\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save model state\"\"\"\n",
    "        model_state = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'history': self.history,\n",
    "            'param_ranges': self.param_ranges,\n",
    "            'alpha': self.alpha,\n",
    "            'context_dim': self.context_dim,\n",
    "            'param_dim': self.param_dim\n",
    "        }\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model_state, f)\n",
    "        print(f\"Model saved to: {path}\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load model state\"\"\"\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                model_state = pickle.load(f)\n",
    "            \n",
    "            # Check dimension compatibility\n",
    "            if model_state['context_dim'] != self.context_dim:\n",
    "                raise ValueError(f\"Model dimension incompatibility: expected facial feature dimension {self.context_dim}, model has {model_state['context_dim']}\")\n",
    "            if model_state['param_dim'] != self.param_dim:\n",
    "                raise ValueError(f\"Model dimension incompatibility: expected glasses parameter dimension {self.param_dim}, model has {model_state['param_dim']}\")\n",
    "            \n",
    "            # Load parameters\n",
    "            self.model.load_state_dict(model_state['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(model_state['optimizer_state_dict'])\n",
    "            self.history = model_state['history']\n",
    "            self.param_ranges = model_state['param_ranges']\n",
    "            self.alpha = model_state['alpha']\n",
    "            print(f\"Model loaded successfully: {path}, total historical samples={len(self.history)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load model: {e}\")\n",
    "\n",
    "\n",
    "def generate_and_show_image(glass_params, start_latent, gvton, input_image=None, title=\"Recommended Glasses\"):\n",
    "    Size, Height, Squareness, Round_Shrink, Cateye, Thicken = glass_params\n",
    "    num_pcs = 6\n",
    "    latent = start_latent.copy()\n",
    "    for PC_num, PC_bias in enumerate((Size, Height, Squareness, Round_Shrink, Cateye, Thicken)):\n",
    "        run_gen = True if PC_num == (num_pcs - 1) else False\n",
    "        img, latent = gvton.e4e.run_gen_add_pc_direction_bias(\n",
    "            start_latent=latent, fitted_pca=gvton.fitted_pca, bias=PC_bias, PC_num=PC_num, run_gen=run_gen\n",
    "        )\n",
    "    if input_image is not None:\n",
    "        blends, _, _ = gvton.blend_in_edits(edits=[img], input_image=input_image)\n",
    "        result_image = blends[0]\n",
    "    else:\n",
    "        result_image = img\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(result_image)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return result_image\n",
    "\n",
    "\n",
    "def create_recommendation_interface(recommender, gvton, start_latent, input_image=None, face_features=None):\n",
    "    \"\"\"Create interactive recommendation interface\"\"\"\n",
    "    output = widgets.Output()\n",
    "    save_output = widgets.Output()\n",
    "    \n",
    "    # Display initial information\n",
    "    with output:\n",
    "        print(\"Glasses recommendation system started\")\n",
    "        if face_features is not None:\n",
    "            print(f\"Loaded facial features (dimension: {len(face_features)})\")\n",
    "        else:\n",
    "            print(\"Warning: No facial features provided (face_features is None)\")\n",
    "    \n",
    "    # Interface components\n",
    "    # Recommendation button\n",
    "    recommend_btn = widgets.Button(description='Get Recommendation', layout=widgets.Layout(width='150px'))\n",
    "    \n",
    "    # Rating buttons (5-level rating)\n",
    "    rating_buttons = [\n",
    "        widgets.Button(\n",
    "            description='Strongly Dislike',\n",
    "            layout=widgets.Layout(width='120px'),\n",
    "            style={'button_color': '#FF6B6B'},  # Dark red\n",
    "            tooltip='Reward: -0.3'\n",
    "        ),\n",
    "        widgets.Button(\n",
    "            description='Dislike',\n",
    "            layout=widgets.Layout(width='120px'),\n",
    "            style={'button_color': '#FFD166'},  # Light red\n",
    "            tooltip='Reward: -0.1'\n",
    "        ),\n",
    "        widgets.Button(\n",
    "            description='Neutral',\n",
    "            layout=widgets.Layout(width='120px'),\n",
    "            style={'button_color': '#FFFFFF', 'text_color': '#000000'},  # White (black text)\n",
    "            tooltip='Reward: 0'\n",
    "        ),\n",
    "        widgets.Button(\n",
    "            description='Like',\n",
    "            layout=widgets.Layout(width='120px'),\n",
    "            style={'button_color': '#66D2A0'},  # Light green\n",
    "            tooltip='Reward: 0.1'\n",
    "        ),\n",
    "        widgets.Button(\n",
    "            description='Strongly Like',\n",
    "            layout=widgets.Layout(width='120px'),\n",
    "            style={'button_color': '#06D6A0'},  # Dark green\n",
    "            tooltip='Reward: 0.3'\n",
    "        )\n",
    "    ]\n",
    "    reward_mapping = [-0.3, -0.1, 0, 0.1, 0.3]  # Rewards corresponding to ratings\n",
    "    \n",
    "    # Glasses parameter fine-tuning sliders\n",
    "    param_sliders = [\n",
    "        widgets.FloatSlider(\n",
    "            min=low, max=high, step=0.5, value=0,\n",
    "            description=name, layout=widgets.Layout(width='500px')\n",
    "        )\n",
    "        for name, (low, high) in zip(recommender.param_names, recommender.param_ranges)\n",
    "    ]\n",
    "    \n",
    "    # Function buttons\n",
    "    apply_btn = widgets.Button(description='Apply Adjustments')\n",
    "    regenerate_btn = widgets.Button(description='Regenerate Image')\n",
    "    \n",
    "    # Model saving controls\n",
    "    save_path = widgets.Text(value='dl_model.pkl', description='Save Path:')\n",
    "    save_btn = widgets.Button(description='Save Model')\n",
    "    \n",
    "    # State variables\n",
    "    current_face_features = face_features\n",
    "    current_params = None\n",
    "    current_score = None\n",
    "    current_image = None\n",
    "    \n",
    "    def on_recommend_click(b):\n",
    "        \"\"\"Callback for \"Get Recommendation\" button click\"\"\"\n",
    "        nonlocal current_params, current_score, current_image\n",
    "        \n",
    "        if current_face_features is None:\n",
    "            with output:\n",
    "                clear_output(wait=True)\n",
    "                print(\"Error: Facial features are empty (face_features is None)\")\n",
    "            return\n",
    "        \n",
    "        # Get recommended parameters\n",
    "        try:\n",
    "            best_params, all_params, all_scores = recommender.recommend(\n",
    "                current_face_features, n_candidates=50, return_all=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            with output:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Recommendation failed: {e}\")\n",
    "            return\n",
    "        \n",
    "        current_params = best_params\n",
    "        current_score = all_scores[np.argmax(all_scores)]\n",
    "        \n",
    "        # Update slider values\n",
    "        for i, s in enumerate(param_sliders):\n",
    "            s.value = best_params[i]\n",
    "        \n",
    "        # Display results\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Generate image\n",
    "            print(\"\\nGenerating recommended glasses image:\")\n",
    "            current_image = generate_and_show_image(\n",
    "                best_params, start_latent, gvton, input_image,\n",
    "                title=f\"Recommended Glasses\"\n",
    "            )\n",
    "            \n",
    "            # Display interactive controls\n",
    "            display(widgets.Label(\"Please rate the recommendation:\"))\n",
    "            display(widgets.HBox(rating_buttons))\n",
    "            display(widgets.Label(\"Adjust parameters:\"))\n",
    "            display(widgets.VBox(param_sliders))\n",
    "            display(widgets.HBox([apply_btn, regenerate_btn]))\n",
    "    \n",
    "    def on_rating_click(b):\n",
    "        \"\"\"Callback for rating button clicks (update model)\"\"\"\n",
    "        nonlocal current_params\n",
    "        if current_params is None or current_face_features is None:\n",
    "            return\n",
    "        \n",
    "        # Get reward value\n",
    "        btn_index = rating_buttons.index(b)\n",
    "        reward = reward_mapping[btn_index]\n",
    "        # Update model\n",
    "        recommender.update(current_face_features, current_params, reward)\n",
    "        # Recommend again\n",
    "        on_recommend_click(None)\n",
    "    \n",
    "    def on_apply_click(b):\n",
    "        \"\"\"Callback for \"Apply Adjustments\" button click\"\"\"\n",
    "        nonlocal current_params, current_score, current_image\n",
    "        \n",
    "        if current_face_features is None:\n",
    "            return\n",
    "        \n",
    "        # Get fine-tuned parameters\n",
    "        tweaked_params = np.array([s.value for s in param_sliders])\n",
    "        current_params = tweaked_params\n",
    "        \n",
    "        # Calculate score after fine-tuning (using model predicted mean)\n",
    "        try:\n",
    "            feature_vector = recommender.build_feature_vector(current_face_features, tweaked_params)\n",
    "        except ValueError as e:\n",
    "            with output:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Adjustment failed: {e}\")\n",
    "            return\n",
    "        \n",
    "        feature_tensor = torch.FloatTensor(feature_vector).unsqueeze(0).to(recommender.device)\n",
    "        with torch.no_grad():\n",
    "            mean, _ = recommender.model(feature_tensor)\n",
    "        current_score = mean.item()\n",
    "        \n",
    "        # Display fine-tuning results\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Adjusted parameters (Predicted mean: {current_score:.4f}):\")\n",
    "            for name, val in zip(recommender.param_names, tweaked_params):\n",
    "                print(f\"  {name}: {val:.2f}\")\n",
    "            \n",
    "            # Generate image\n",
    "            print(\"\\nGenerating adjusted glasses image:\")\n",
    "            current_image = generate_and_show_image(\n",
    "                tweaked_params, start_latent, gvton, input_image,\n",
    "                title=f\"Adjusted Glasses\"\n",
    "            )\n",
    "            \n",
    "            # Display interactive controls\n",
    "            display(widgets.Label(\"Please rate the adjusted result:\"))\n",
    "            display(widgets.HBox(rating_buttons))\n",
    "            display(widgets.Label(\"Adjust parameters:\"))\n",
    "            display(widgets.VBox(param_sliders))\n",
    "            display(widgets.HBox([apply_btn, regenerate_btn]))\n",
    "    \n",
    "    def on_regenerate_click(b):\n",
    "        \"\"\"Callback for \"Regenerate Image\" button click\"\"\"\n",
    "        nonlocal current_image\n",
    "        if current_params is None:\n",
    "            return\n",
    "        \n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Regenerating image:\")\n",
    "            current_image = generate_and_show_image(\n",
    "                current_params, start_latent, gvton, input_image\n",
    "            )\n",
    "            display(widgets.Label(\"Please rate the image:\"))\n",
    "            display(widgets.HBox(rating_buttons))\n",
    "            display(widgets.Label(\"Adjust parameters:\"))\n",
    "            display(widgets.VBox(param_sliders))\n",
    "            display(widgets.HBox([apply_btn, regenerate_btn]))\n",
    "    \n",
    "    def on_save_click(b):\n",
    "        \"\"\"Callback for \"Save Model\" button click\"\"\"\n",
    "        with save_output:\n",
    "            clear_output()\n",
    "            try:\n",
    "                recommender.save_model(save_path.value)\n",
    "            except Exception as e:\n",
    "                print(f\"Saving failed: {e}\")\n",
    "    \n",
    "    # Bind events\n",
    "    recommend_btn.on_click(on_recommend_click)\n",
    "    for btn in rating_buttons:\n",
    "        btn.on_click(on_rating_click)\n",
    "    apply_btn.on_click(on_apply_click)\n",
    "    regenerate_btn.on_click(on_regenerate_click)\n",
    "    save_btn.on_click(on_save_click)\n",
    "    \n",
    "    # Interface layout\n",
    "    input_box = widgets.VBox([recommend_btn])\n",
    "    layout = widgets.VBox([\n",
    "        widgets.HBox([input_box, output]),\n",
    "        widgets.HBox([save_path, save_btn, save_output])\n",
    "    ])\n",
    "    \n",
    "    display(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be60eb67-8d9d-4099-b42f-22cf9cea3088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0dfc68e56b42048d3bfd4fcca880fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Button(description='Get Recommendation', layout=Layout(width='150…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建推荐器\n",
    "    recommender = GlassesRecommender(alpha=0.2)\n",
    "\n",
    "    create_recommendation_interface(\n",
    "         recommender=recommender,\n",
    "         gvton=gvton,\n",
    "         start_latent=start_latent,\n",
    "         face_features=face_features\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67793990-0ed5-4d0a-9a30-5ca5dd55bde8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
